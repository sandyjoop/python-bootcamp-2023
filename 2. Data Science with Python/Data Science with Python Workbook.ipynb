{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd65d45",
   "metadata": {},
   "source": [
    "# Data Science in Python\n",
    "\n",
    "Python is a great general purpose, high-level programming language and one of its highlights is its ability to use well-developed packages for data science. With simple to learn syntax rules and highly developed support forums Python makes it easier for you to prepare your data prior to feeding it into your data science applications. \n",
    "\n",
    "The main learning outcomes of this session are:\n",
    "- **Importing data and using dataframes**: Importing data using the `pandas` library and manipulating data efficiently \n",
    "- **Basic exploratory data anlysis using Python**: Learning the basics of looking at our data and cleaning it up prior to drawing any conclusions and trying to build analysis - comparing imputation techniques\n",
    "- **Application of data analysis to an example dataset**: Examining all of the above on a toy dataset that has been created for the purpose of this workshop\n",
    "\n",
    "This session particularly will focus on using the following libraries in the data science context:\n",
    "- **Numpy**: A library with a large collection of high-level mathematical functions, and support for large, multidimensional arrays/matrices. In particular it works very well in tandem with pandas. [numpy page](https://numpy.org/)\n",
    "- **Pandas**: A library particularly developed for data manipulation and analysis, offering efficient data structures and operations. In particular it works very well in tandem with numpy. [pandas page](https://pandas.pydata.org/)\n",
    "- **Scikit-Learn**: A library featuring various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN. [scikit-learn page](https://scikit-learn.org/stable/)\n",
    "\n",
    "The key themes overlaying all of the above content are:\n",
    "- **Doing things efficiently**: Efficiency in Python is not just about writing code quickly but also about optimising it for performance and resource consumption. This workshop aims to show you how to do things correctly the first time.\n",
    "- **Learning best practices**: Python has a set of coding standards and guidelines to ensure consistency and maintainability of code which will be reflected throughout this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4dc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install dependencies before running workshop\n",
    "!pip3 install pandas\n",
    "!pip3 install numpy\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install matplotlib\n",
    "!pip3 install opencv-python\n",
    "!pip3 install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200ac87",
   "metadata": {},
   "source": [
    "# Importing data and using dataframes\n",
    "\n",
    "One of the great things about using `pandas` is that it enable us to easily import and manipulate data. In fact when we pair it up with `numpy` it allows us to do transformations and other manipulations of our data to allow us to do our analytics work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries and give them aliases so they're easier to use\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_data = pd.read_excel('Simpsons Data.xlsx',sheet_name = 'Finance Data', index_col = 0)\n",
    "hr_data = pd.read_excel('Simpsons Data.xlsx',sheet_name = 'HR Data', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our finance data\n",
    "finance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our HR data\n",
    "hr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dce353",
   "metadata": {},
   "source": [
    "## Selecting data from dataframes\n",
    "One very important thing to understand about dataframes is that they operate differently from lists or arrays. Dataframes have indexes which is how each row is mapped. Here we have used the employee names as the index, but if we don't specify anything, the default index `pandas` creates is using integers.\n",
    "\n",
    "When selecting data, we can either address columns by name, or by their row/column index to select columns, rows, or their intersection.\n",
    "\n",
    "To **select a column**, we can either address it by name or through its column index.\n",
    "- Addressing by name is done through `data_frame[\"column_name\"]`\n",
    " - Or for a span of columns `data_frame.loc[:, \"start_column\":\"end_column\"]`\n",
    "- Addressing by column index is done through `data_frame.iloc[:, column_index_number]` \n",
    "\n",
    "To **select an entire row**, we can either directly address them using the index, or we can select them by setting a criteria for another column and extracting those rows.\n",
    "- Addressing by index row is done through `data_frame.iloc[data_frame.index==index_row]`\n",
    "- Addressing by condition is done through a similar method `data_frame.loc[boolean condition]` with the boolean condition being in relation to the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85383e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the Age column from HR data\n",
    "print(hr_data[\"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611b4d9-2174-4ad3-a169-d01c5c98cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or doing the same thing through column index\n",
    "print(hr_data.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking our boolean condition for row selection. We want to check rows where age <40 is True or False\n",
    "hr_data[\"Age\"]<40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the rows of HR data where they are <40\n",
    "hr_data.loc[hr_data[\"Age\"]<40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ede1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the row where the index is \"Homer Simpson\"\n",
    "finance_data.iloc[finance_data.index==\"Homer Simpson\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Select all rows where income is >= $65,000 per year\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2025bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension: Return the age of all employees whose income is < $70,000 per year\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671906db",
   "metadata": {},
   "source": [
    "## Merging and adding data to dataframes\n",
    "We can also **concatenate dataframes** by using their index as a matching key to merge data, or add columns and rows to existing dataframes.\n",
    "- To merge dataframes we use `pd.concat(list_of_dfs)`\n",
    " - We can also specify the `axis` argument to choose whether we join horizontally (`axis = 1`) or vertically stack (`axis = 0` - the default)\n",
    " - Note where the indexes match they will automatically merge. Where we want to define a new index, we can create this by specifying `ignore_index=True` \n",
    "- To add to dataframes **efficiently** we generally want to define the data we want to add as a dictionary, which we convert to a dataframe and then concatenate using the above method\n",
    " - To add rows we can use `pd.DataFrame.from_dict(new_dict, orient='index', columns = df_headers_list)`\n",
    " - To add columns we `pd.DataFrame.from_dict(new_dict)` where the key of our dictionary is the column name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca306e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge our data into a single dataframe\n",
    "employee_df = pd.concat([finance_data, hr_data], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f94b11-ce6e-443d-906b-98a073ae53dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df # check our concat has worked as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f87fe-3678-4dd4-afb2-6cdd9df8ddb1",
   "metadata": {},
   "source": [
    "We have 2 new employees who are joining the company and need their details added to the `employee` dataframe above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of our headers to see what we need to fill out\n",
    "headers = employee_df.columns.tolist() # ['Income', 'Position', 'Age', 'Comments']\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7dcdd-3c3e-4059-b8b9-95b271ab5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our new employee data as a dictionary\n",
    "new_employees = {\"Hans Moleman\": [10, \"HR\", 100, \"Has cataracts\"],\\\n",
    "                 \"Edna Krabappel\" : [32.5, \"Knowledge Management\", 32, \"Smoking risk\"]}\n",
    "\n",
    "new_employees_df = pd.DataFrame.from_dict(new_employees, orient='index', columns=headers)\n",
    "new_employees_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a044f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise - Add the new employees to the original employee dataframe\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95660119",
   "metadata": {},
   "source": [
    "## Modifying data in dataframes\n",
    "Selecting data from a dataframe  generates what is known as a **slice** of the dataframe which is a link to the original data.  This is fine if we just want to view what we are working with, but if we ever want to modify data, we need to generate a copy of a slice to work with using `data_frame_slice.copy()`.\n",
    "\n",
    "We can do normal mathematical operations on these columns using numpy, or just using normal mathematical operators, and replace the original data with our transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income data is in thousands, let's convert it to the correct raw number\n",
    "employee_df[\"Income\"] = employee_df[\"Income\"].copy()*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: We've been given employee gender, try adding this as a column to employee_df \n",
    "employee_gender = {\"Gender\" : [\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"M\",\"F\"]}\n",
    "\n",
    "# Create dataframe of employee gender\n",
    "'''Your Code Here'''\n",
    "\n",
    "# Concatenate dfs\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173846dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise - Add a new column to the dataframe with the height of each employee in cm (hint 1 inch = 2.54 cm)\n",
    "employee_height = {\"Height\" : [68,60,62,75,65,70,67,50,60]}\n",
    "\n",
    "# Create dataframe of height\n",
    "'''Your Code Here'''\n",
    "\n",
    "# Transform height to cm\n",
    "'''Your Code Here'''\n",
    "\n",
    "# Concatenate dfs\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f9f4b",
   "metadata": {},
   "source": [
    "# Basic Exploratory Data Analysis\n",
    "\n",
    "Exploratory data analysis (EDA) is one of the other key tools that we have in our toolkit as data scientists. EDA involves 2 main actions:\n",
    "1. Understanding how good (or more usually how bad) your data is\n",
    "     - Understanding what data means\n",
    "     - Understanding where data is missing and whether it's important\n",
    "     - Is there enough data to draw meaningful conclusions?\n",
    "     - Are there outliers, is this important?\n",
    "\n",
    "2. Where possible, modifying data to make analysis more meaningful\n",
    "     - Filling in blanks where appropriate, using the best method\n",
    "     - Normalising data to ensure uniform variance\n",
    "     - Converting data to the appropriate type\n",
    " \n",
    "The easiest way to start doing this is to use an example, as this shows how easy it is to explore a dataset, but also how easy it is to fix our data issues. Let's take a look at two datasets which come built into scikit-learn. \n",
    "\n",
    "The diabetes dataset has 442 entries, each with 10 features. The California Housing dataset is much larger with 20,640 entries and 8 features. Keep in mind that **MedInc** are in units of 10,000, while **MedHouseVal** are in units of 100,000. We will only use the first 300 entries for both datasets for the sake of speeding up the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0d3a2-42bd-4408-9554-72dbb7dd45af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes # import toy datasets from sklearn\n",
    "from sklearn.datasets import fetch_california_housing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9b191-d130-4ad2-9370-1dd48c13cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes(as_frame=True)\n",
    "diabetes_df = diabetes.frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25673955",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc275c0d-6bff-4b2b-9040-3ae774f7d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.head().round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40971b4d-c38f-423a-abd2-367294ad32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d6cfd-16e9-42af-9ded-8d150a1620f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the california housing dataset \n",
    "'''Your Code Here'''\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows and columns of the dataset\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20689cd-2d7f-4939-bc51-e3adf77a734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the first 5 rows of the DataFrame and round to 3 decimal places\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb33e4-6028-44ee-857f-b528fd4fa5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve descriptive statistics and round to 3 decimal places\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve descriptive statistics for the median house valie and round to 3 decimal places\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f48ec66-e436-492d-8766-86f73a7ce989",
   "metadata": {},
   "source": [
    "## Add missing values to the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a87f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42) # Choose a seed so that we all get similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e2256-b766-4be0-986a-c0205744f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_diabetes, y_diabetes = load_diabetes(return_X_y=True) # Return X and y data as 2 arguments to unpack instead of 1\n",
    "X_california, y_california = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "# Take the first 300 values of both datasets\n",
    "X_california = X_california[:300]\n",
    "y_california = y_california[:300]\n",
    "X_diabetes = X_diabetes[:300] \n",
    "y_diabetes = y_diabetes[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe08f1-384f-4d5f-bfa7-cfc2cb1f0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to add in missing values, with a default drop rate of 75%\n",
    "def add_missing_values(X_full, y_full, missing_rate = 0.75):\n",
    "    n_samples, n_features = X_full.shape\n",
    "\n",
    "    n_missing_samples = int(n_samples * missing_rate) # Calc the number of samples which we will remove\n",
    "\n",
    "    missing_samples = np.zeros(n_samples, dtype=bool) # Pre-allocate matrix for speed\n",
    "    missing_samples[:n_missing_samples] = True # Select how many samples to drop\n",
    "\n",
    "    rng.shuffle(missing_samples) # Randomly shuffle the truth mask for the samples\n",
    "    missing_features = rng.randint(0, n_features, n_missing_samples) # Select which feature is dropped at random for each sample\n",
    "    X_missing = X_full.copy()\n",
    "    X_missing[missing_samples, missing_features] = np.nan # Drop the selected features and replace with NaN\n",
    "    y_missing = y_full.copy()\n",
    "\n",
    "    return X_missing, y_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b07ca7-01f2-4a64-8a51-739cb9feb84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the california housing and diabetes datasets\n",
    "X_miss_california, y_miss_california = add_missing_values(X_california, y_california)\n",
    "X_miss_diabetes, y_miss_diabetes = add_missing_values(X_diabetes, y_diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_miss_california"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30fc32",
   "metadata": {},
   "source": [
    "## Comparing imputation techniques across datasets\n",
    "\n",
    "There are a lot of ways to impute missing datasets, each which have their own benefits and downsides. As data scientists, we need to know which one works in what circumstance. Let's compare techniques using scikit-learn's inbuilt imputer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c819f-5786-49ee-a33b-0d444478d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd86924-d390-4687-8c85-2c3cf6d36289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for k-fold cross-validation\n",
    "n_splits = 4\n",
    "regressor = RandomForestRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1320db7e",
   "metadata": {},
   "source": [
    "###  Calculating the performance of imputation using cross validation\n",
    "When we are doing comparison we need to figure out how it performs. One method to do this with small datasets is using cross validation scores.\n",
    "\n",
    "Cross validation scores are calculated by splitting the training set into k smaller sets. The following procedure is followed for each of the k “folds”:\n",
    "- A model is trained using `k-1` of the folds as training data;\n",
    "- The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop giving an estiamte of accuracy.\n",
    "![alt text](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the performance of an imputer\n",
    "def get_scores_for_imputer(imputer, X_missing, y_missing):\n",
    "    estimator = make_pipeline(imputer, regressor)\n",
    "    impute_scores = cross_val_score(\n",
    "        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=n_splits\n",
    "    )\n",
    "    return impute_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c8cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = [] # Labels for when we want to graph our results\n",
    "\n",
    "# Pre-allocate our MSE and standard deviation result matrices for each dataset\n",
    "mses_diabetes = np.zeros(4)\n",
    "stds_diabetes = np.zeros(4)\n",
    "mses_california = np.zeros(4)\n",
    "stds_california = np.zeros(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab25f8",
   "metadata": {},
   "source": [
    "## Reference: Calculate cross validation loss with the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617651df-3157-4d28-9ede-ad2cf15c90f5",
   "metadata": {},
   "source": [
    "The function below evaluates the performance of a regression model (represented by regressor) on the full datasets for the diabetes and california housing datasets, and is for reference only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb4acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_score(X_full, y_full):\n",
    "    full_scores = cross_val_score(\n",
    "        regressor, X_full, y_full, scoring=\"neg_mean_squared_error\", cv=n_splits\n",
    "    )\n",
    "    return abs(full_scores.mean()), abs(full_scores.std())\n",
    "\n",
    "\n",
    "mses_california[0], stds_california[0] = get_full_score(X_california, y_california)\n",
    "mses_diabetes[0], stds_diabetes[0] = get_full_score(X_diabetes, y_diabetes)\n",
    "x_labels.append(\"Full data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551191a4",
   "metadata": {},
   "source": [
    "## Replacing all missing values with 0s\n",
    "One option is to replace all our values with 0s. Given the average for most of the featues in our dataset is 0, this may not be a bad option. However, because this is synthetically modifying data, we are changing the distribution of the data. Where the mean is close to 0 this may not be as big an issue, but where this isn't the case it can have **big** impacts on our data distrubtion and turn our data into garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impute_zero_score(X_missing, y_missing):\n",
    "\n",
    "    imputer = SimpleImputer(\n",
    "        missing_values=np.nan, add_indicator=True, strategy=\"constant\", fill_value=0\n",
    "    )\n",
    "    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n",
    "    return abs(zero_impute_scores.mean()), abs(zero_impute_scores.std())\n",
    "\n",
    "\n",
    "mses_california[1], stds_california[1] = get_impute_zero_score(\n",
    "    X_miss_california, y_miss_california\n",
    ")\n",
    "mses_diabetes[1], stds_diabetes[1] = get_impute_zero_score(\n",
    "    X_miss_diabetes, y_miss_diabetes\n",
    ")\n",
    "\n",
    "x_labels.append(\"Zero imputation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f7192f",
   "metadata": {},
   "source": [
    "## Imputing missing values using k-Nearest Neighbours (kNN)\n",
    "\n",
    "Given we are an AI practice, why can't we use AI to fill in the data! But in all seriousness, one method which is a good way to retain the data distribution and repair our dataset is to impute data, based on the k nearest neighbours of each datapoint. \n",
    "\n",
    "This is done usually by calculating the total euclidean distance between each point based on their features and then minimising that for the k nearest points for the missing datapoint. Generally, this is rarely an issue, **except for where we have a _lot_ of data due to computational expense of kNN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impute_knn_score(X_missing, y_missing):\n",
    "    \n",
    "    imputer = KNNImputer(missing_values=np.nan, add_indicator=True)\n",
    "    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n",
    "    return abs(knn_impute_scores.mean()), abs(knn_impute_scores.std())\n",
    "\n",
    "\n",
    "mses_california[2], stds_california[2] = get_impute_knn_score(\n",
    "    X_miss_california, y_miss_california\n",
    ")\n",
    "mses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(\n",
    "    X_miss_diabetes, y_miss_diabetes\n",
    ")\n",
    "x_labels.append(\"KNN Imputation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312277b1",
   "metadata": {},
   "source": [
    "## Imputing missing values using the mean\n",
    "\n",
    "Another option is to just use the mean. Ideally this will just make the distribution of data narrower, however, generally this is a better option than replacing with 0s. \n",
    "\n",
    "Where this is not suited is **where there are a lot of outliers** in which case using the median may be a better option. Given this is not the case with either dataset, we should see okay performance from this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impute_mean(X_missing, y_missing):\n",
    "    \n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\", add_indicator=True)\n",
    "    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n",
    "    return abs(mean_impute_scores.mean()), abs(mean_impute_scores.std())\n",
    "\n",
    "\n",
    "mses_california[3], stds_california[3] = get_impute_mean(\n",
    "    X_miss_california, y_miss_california\n",
    ")\n",
    "mses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\n",
    "x_labels.append(\"Mean Imputation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c69530",
   "metadata": {},
   "source": [
    "## Plotting to compare results\n",
    "\n",
    "Let's look at the results in a visual. Fortunately matplotlib provides easy to use visualisations to do this. Note that because we are evaluating these results using cross validation scores, we won't see a 0 MSE loss for even the original dataset. However, generally the lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e825f-9fa4-4fbe-852c-89c5b987c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bars = len(mses_diabetes)\n",
    "xval = np.arange(n_bars)\n",
    "\n",
    "colors = [\"r\", \"g\", \"b\", \"orange\", \"black\"]\n",
    "\n",
    "# plot diabetes results\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax1 = plt.subplot(121)\n",
    "for j in xval:\n",
    "    ax1.barh(\n",
    "        j,\n",
    "        mses_diabetes[j],\n",
    "        xerr=stds_diabetes[j],\n",
    "        color=colors[j],\n",
    "        alpha=0.6,\n",
    "        align=\"center\",\n",
    "    )\n",
    "\n",
    "ax1.set_title(\"Imputation Techniques with Diabetes Data\")\n",
    "ax1.set_xlim(left=np.min(mses_diabetes) * 0.9, right=np.max(mses_diabetes) * 1.1)\n",
    "ax1.set_yticks(xval)\n",
    "ax1.set_xlabel(\"MSE\")\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_yticklabels(x_labels)\n",
    "\n",
    "# plot california dataset results\n",
    "ax2 = plt.subplot(122)\n",
    "for j in xval:\n",
    "    ax2.barh(\n",
    "        j,\n",
    "        mses_california[j],\n",
    "        xerr=stds_california[j],\n",
    "        color=colors[j],\n",
    "        alpha=0.6,\n",
    "        align=\"center\",\n",
    "    )\n",
    "\n",
    "ax2.set_title(\"Imputation Techniques with California Data\")\n",
    "ax2.set_yticks(xval)\n",
    "ax2.set_xlabel(\"MSE\")\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_yticklabels([\"\"] * n_bars)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7e318",
   "metadata": {},
   "source": [
    "In both datasets, kNN is the best performer, followed by mean imputation, then zero imputation. This outcome is somewhat expected, but may be due to many reasons.\n",
    " \n",
    " Both diabetes and california housing datasets may have complex and non-linear relationships between the features. KNN imputation can capture such relationships effectively because it leverages the similarity of data points to fill in missing values. KNN can identify and use nearby data points to make informed imputations. It's also a flexible approach because it doesn't make strong assumptions about the data's distribution.\n",
    "\n",
    " Mean imputation is a baseline method but can be less effective when there are complex relationships between data, or non-linear.\n",
    "\n",
    " Zero imputation is the simplest method and often oversimplifies the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ff0fa5",
   "metadata": {},
   "source": [
    "# Using Numpy\n",
    "\n",
    "NumPy is an incredibly powerful tool to do numerical manipulation and modification within Python. Because most of the code base is written in C it is also (usually) a lot faster at doing certain operations than the inbuilt Python functions.\n",
    "\n",
    "Much of this speed gain also comes from using NumPy arrays instead of Python lists. NumPy arrays are faster and more compact than Python lists. An array consumes less memory and is convenient to use. NumPy uses much less memory to store data and it provides a mechanism of specifying the data types. This allows the code to be optimized even further.\n",
    "\n",
    "To explore the functionality of `NumPy` we'll be going through an exercise of how to use it to do:\n",
    "- Basic array manipulation - inducing, slicing, slicing based on a boolean condition\n",
    "- Basic linear algebra with arrays - multiplication by a constant, dot product, matrix multiplication\n",
    "- Data types and casting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe87ee5",
   "metadata": {},
   "source": [
    "## Basic array manipulation\n",
    "One of the advantages of NumPy is that it is very good at allowing us to do array manipulation. This is particularly useful in ML applications, but it also has use where we are just dealing with normal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd8676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with how to make an array\n",
    "x = np.array([[[1 , 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\\\n",
    "              [[13 , 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910340e-9215-43a9-924c-6871f29790ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of x is:\")\n",
    "print(x.shape) # note that this is a 3 dimensional array\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can turn this into a 1-dimensional array by using the .flatten() method\n",
    "# This is useful for when we are taking our final layer of a neural net and feeding it into a softmax layer\n",
    "print(x.flatten())\n",
    "print(\"The shape of flattened x is:\")\n",
    "print(x.flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb56b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a94911",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = np.arange(6) # generate an array that has 6 elements, starting from 0 and increasing in increments of 1\n",
    "toy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd4f069",
   "metadata": {},
   "source": [
    "### Numpy Broadcasting\n",
    "One of the things that makes numpy great as a mathematical library is its ability to do very efficient array and matrix operations. One way that it can do this is through a special function called broadcasting, that allows you to perform element-wise operations on arrays with different shapes.\n",
    "\n",
    "Let's check it out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc23741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting via addition - add dimensions with each column having a different operation on it\n",
    "modified_data = toy_data[:, np.newaxis] + np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcefe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine toy_data[:, np.newaxis]\n",
    "toy_data[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01140ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b53730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting via multiplication/division - note inner dimensions must match\n",
    "working_multipliers = np.array([0.5,-1.2,1])\n",
    "mismatch_multipliers = np.array([0.5,-1.2,1,0])\n",
    "\n",
    "print(\"Working Multiplication:\")\n",
    "print(modified_data * working_multipliers)\n",
    "\n",
    "modified_data * mismatch_multipliers # This does not work because NumPy cannot map the 4 dimensions onto a 3 column matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4691768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise - will mismatch multipliers work with our original matrix, x? \n",
    "# Try out below, and try to explain what happens\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94dfdab",
   "metadata": {},
   "source": [
    "### Putting it all together in an example: Working out customer satisfaction \n",
    "One of the key reasons NumPy is the bread and butter of many data science applications in Python is because it's _really_ good at doing matrix operations. Once we realise this we can do things, we can exploit the way that we do our data preparation and exploration to get to outcomes very quickly.\n",
    "\n",
    "Some of the key functions in NumPy are:\n",
    "- Matrix multiplication using `.matmul()`\n",
    "- Getting the dot product of two matrixes by the dot product using `.dot()`\n",
    "- Getting a vector cross product using `.cross()`\n",
    "\n",
    "To explore this we are going to go through an example of dealing with user experience data from feedback froms from our hypothetical website. Let us assume we get **100 survey** results from respondents, each who have answered **10 questions**. We'll be using the `.dot()` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data = np.random.random(1000) # generate random samples\n",
    "print(\"The shape of our original data is: \" + str(survey_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26d3b6-6713-4c44-8512-a549f89bd060",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data = survey_data.reshape((100,10)) # reshape our data so that each respondent has their own array\n",
    "print(\"The shape of our modified data is: \" + str(survey_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c7bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc94c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets normalise our data so that we can do fair data analysis\n",
    "survey_data = (survey_data - survey_data.min())/(survey_data.max() - survey_data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data # Compare to the above non-normalised data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abd2ca2",
   "metadata": {},
   "source": [
    "Now that we have our data in a useful form that we actually do things with (and we can compare the results of each survey now that they are normalised), we can start drawing insights from our data.\n",
    "\n",
    "The client's marketing team has come to us and said that for their next marketing campaign they're want to focus on the UX aspects of their landing page, which was surveyed in questions 1, 2, 5 and 7 out of the 10 questions. They want a baseline result of average customer satisfaction across the cohort to measure performance against.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a boolean array to only select the values we want from our survey\n",
    "marketing_value = np.array([1,1,0,0,1,0,1,0,0,0],dtype=bool) # Select the questions we are interested in\n",
    "marketing_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9940d1-3c95-41fa-af7d-ec232f92274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply our boolean mask to our survey data to get our average baseline score for each customer\n",
    "marketing_outcomes = np.dot(survey_data,marketing_value) # this takes the dot product out [100,10] x [10,1] matrix to give a [100 x 1] output of raw results\n",
    "\n",
    "print(marketing_outcomes.shape) # Confirm dimensionality is correct\n",
    "print(marketing_outcomes) # Look at the output - what does each number represent here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1611f-9409-4330-8e1c-64d733133002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Get average score and scale to be a %\n",
    "sum(marketing_value)\n",
    "marketing_outcomes_scaled = (marketing_outcomes / sum(marketing_value)) * 100\n",
    "marketing_outcomes_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f87c0-1a90-42da-a498-02654c279805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Get the average customer satisfaction for the marketing team\n",
    "print(f\"Average Marketing Satisfaction: {np.mean(marketing_outcomes_scaled)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280cdb4",
   "metadata": {},
   "source": [
    "Now its your turn. \n",
    "\n",
    "The client's engineering team has come to us and said that for their next marketing campaign they're want to focus on the mechanics of the product. However, being engineers, they've put relative weightings on their preferencing of questions to figure out baseline performance. They think questions 1 and 10 are 3 times more important than the rest, and that questions 2, 5, and 6 are half as important.\n",
    "\n",
    "Figure out the baseline performance for the engineering team to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290aadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create an array of weighted values for each question\n",
    "engineering_value = np.array(['''Your Code Here'''])\n",
    "engineering_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd08d5-5cef-415a-b945-8f9c3ab2a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply our boolean mask to our survey data to get our average baseline score for each customer\n",
    "engineering_outcomes = '''Your Code Here'''\n",
    "\n",
    "print(engineering_outcomes.shape) # Confirm dimensionality is correct\n",
    "print(engineering_outcomes) # Each number represents baseline performance for the engineering team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e311b-c23c-40e3-b235-001347181c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Get average score and scale to be a %\n",
    "engineering_outcomes_scaled = '''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef900e-0edd-4d01-b689-27a4ee532b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Get the average customer satisfaction for the engineering team\n",
    "'''Your Code Here'''\n",
    "print(f\"Average Engineering Satisfaction: {str('Your Code Here')}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ae5e2",
   "metadata": {},
   "source": [
    "The engineering team has engaged Deloitte to help them fix up their tech stack and the engagement team has just finished up. The engineers are suspicious of this score, and they want to test it themselves. They asked the same survey respondents to fill out surveys again and they want to see if average engineering satisfaction has actually gone up.\n",
    "\n",
    "Using the survey data, show whether the Deloitte team did a good job or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f5a3d-343f-437b-b8d3-49f8dae4d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skewnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88895f0-002a-4e48-9fa2-b1edd63c3aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_survey_data = np.concatenate((np.random.normal(0.5,0.1,200),np.random.normal(0.9,0.01,800)),axis=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d4b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f445a-4ea8-4b33-9e56-1222831bc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise our data\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b744b2de-8ecf-4de9-b84a-805c1996b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the engineering mask we generated in our previous exercise\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612a7f6-5993-4fab-a30d-12e0cfa0a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average score and scale to be a %\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c8715-8f46-4c53-a5f9-29b15deab24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average customer satisfcation for the engineering team\n",
    "'''Your Code Here'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff03f52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extension - PCA and image information\n",
    "\n",
    "Often instead of being faced with the issue of there being insufficient data for the purpose of doing analytics we are faced with the issue that we have too much information which is making our analytics slow, or is creating more noise than meaningful information.\n",
    "\n",
    "As good data scientists we can take an intelligent approach to refining our data down into fewer elements which still capture the key variations within our dataset. One of these techniques is called Principal Component Analysis or PCA.\n",
    "\n",
    "PCA works (via some complex mathematics that can be found [here](https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643#:~:text=Introduction,present%20in%20the%20data%20set.)) to reduce down our data into its key components. Doing this with numerical data is often hard to see so instead we're going to be doing it with an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2\n",
    "from scipy.stats import stats\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(cv2.imread('baby-yoda.jpeg'), cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c405ff0b-b756-482a-9750-de79835663b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image shape is: \"+str(img.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5fa66-b3ef-4ce4-9d11-198482bd49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into RGB channels\n",
    "blue,green,red = cv2.split(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9312f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the images to see the different channels\n",
    "fig = plt.figure(figsize = (15, 7.2)) \n",
    "fig.add_subplot(131)\n",
    "plt.title(\"Blue Channel\")\n",
    "plt.imshow(blue, cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "fig.add_subplot(132)\n",
    "plt.title(\"Green Channel\")\n",
    "plt.imshow(green, cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "fig.add_subplot(133)\n",
    "plt.title(\"Red Channel\")\n",
    "plt.imshow(red, cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a1087",
   "metadata": {},
   "source": [
    "Note that the way this colour image is comprised is by having 3 channels - red, green, and blue - for each pixel in the image, with the intensity of each colour being defined as a number between 0 and 255. Each colour channel can be thought of as a matrix, each having a dimension of 551 x 980.\n",
    "\n",
    "Let's confirm this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50031a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_temp_df = pd.DataFrame(data = blue)\n",
    "blue_temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b538456",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f024e70",
   "metadata": {},
   "source": [
    "### Time to find out how much we can compress the image\n",
    "\n",
    "Now that we've explored our image, let's see how much information we can take out before we notice a quality hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff22db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise data first so that it varies between 0 and 1 to create consistent variance and simplify computation\n",
    "df_blue = blue/255\n",
    "df_green = green/255\n",
    "df_red = red/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3abc73-07b9-444a-8acf-01e500b42bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first number of principal components of each image\n",
    "num_comp = 50\n",
    "\n",
    "# Apply PCA to the blue channel\n",
    "pca_b = PCA(n_components=num_comp)\n",
    "pca_b.fit(df_blue)\n",
    "trans_pca_b = pca_b.transform(df_blue)\n",
    "\n",
    "# Apply PCA to the green channel\n",
    "pca_g = PCA(n_components=num_comp)\n",
    "pca_g.fit(df_green)\n",
    "trans_pca_g = pca_g.transform(df_green)\n",
    "\n",
    "# Apply PCA to the red channel\n",
    "pca_r = PCA(n_components=num_comp)\n",
    "pca_r.fit(df_red)\n",
    "trans_pca_r = pca_r.transform(df_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d1bee",
   "metadata": {},
   "source": [
    "What we have done is we have reduced our image into being defined by 50 values which can be used to recreate our image instead of the original 551 (so we've **compressed our image by over 90%!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trans_pca_b.shape)\n",
    "print(trans_pca_r.shape)\n",
    "print(trans_pca_g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47eeb1",
   "metadata": {},
   "source": [
    "Let's see what that actually means though. We can examine this through three steps:\n",
    "- **Explained variance**: This can tell us what % of the variation of our data is captured by the number of components we've chosen\n",
    "- **Histogram of variance ratio**: This shows us how important each of those components are in a visual way\n",
    "- **What our image actually looks like**: The good thing with images is we can see what difference this actually has made to our image qualitatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04736e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance\n",
    "print(f\"Blue Channel : {sum(pca_b.explained_variance_ratio_)*100}%\")\n",
    "print(f\"Green Channel: {sum(pca_g.explained_variance_ratio_)*100}%\")\n",
    "print(f\"Red Channel  : {sum(pca_r.explained_variance_ratio_)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a histogram of the variance for each individual component\n",
    "fig = plt.figure(figsize = (15, 7.2)) \n",
    "\n",
    "fig.add_subplot(131)\n",
    "plt.title(\"Blue Channel\")\n",
    "plt.ylabel('Variation explained')\n",
    "plt.xlabel('Eigenvalue')\n",
    "plt.bar(list(range(1,51)),pca_b.explained_variance_ratio_)\n",
    "\n",
    "fig.add_subplot(132)\n",
    "plt.title(\"Green Channel\")\n",
    "plt.ylabel('Variation explained')\n",
    "plt.xlabel('Eigenvalue')\n",
    "plt.bar(list(range(1,51)),pca_g.explained_variance_ratio_, color='green')\n",
    "\n",
    "fig.add_subplot(133)\n",
    "plt.title(\"Red Channel\")\n",
    "plt.ylabel('Variation explained')\n",
    "plt.xlabel('Eigenvalue')\n",
    "plt.bar(list(range(1,51)),pca_r.explained_variance_ratio_, color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736f3fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Undoing our PCA transform using only the number of components selected\n",
    "b_arr = pca_b.inverse_transform(trans_pca_b)\n",
    "g_arr = pca_g.inverse_transform(trans_pca_g)\n",
    "r_arr = pca_r.inverse_transform(trans_pca_r)\n",
    "print(f\"Undoing our PCA transformation, each channel is of shape : {b_arr.shape, g_arr.shape, r_arr.shape}\")\n",
    "\n",
    "# Merging each colour channel back into one image - the same as before\n",
    "img_reduced= (cv2.merge((b_arr, g_arr, r_arr)))\n",
    "print(f\"The size of our merged image : {img_reduced.shape}\")\n",
    "\n",
    "# Showing the compressed image\n",
    "fig = plt.figure(figsize = (10, 7.2)) \n",
    "fig.add_subplot(121)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(img)\n",
    "fig.add_subplot(122)\n",
    "plt.title(\"Reduced Image\")\n",
    "plt.imshow(img_reduced)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae0cd73",
   "metadata": {},
   "source": [
    "## Extension Questions\n",
    "\n",
    "As you can see even with >90% compression our eyes can't really tell the difference between the original and the compressed image. \n",
    "\n",
    "Questions for your exploration:\n",
    "- How many components can you reduce `num_comp` to before you can tell the difference between the compressed and original image?\n",
    "- Is the information representation uniform between the channels?\n",
    "- What happens if you change the number of components for each channel? Is the colour balance the same?\n",
    "- Super extension question - how can we apply PCA to our city data above? What preliminary preparation would we need to do?\n",
    "\n",
    "For an application of PCA to a real dataset, check out this [link](https://www.kaggle.com/nirajvermafcb/principal-component-analysis-explained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e2d78-c780-4b99-8522-95c16a042be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
